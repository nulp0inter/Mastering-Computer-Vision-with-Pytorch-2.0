{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37294424",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/ava-orange-education/Mastering-Computer-Vision-with-PyTorch-2.0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb1bc8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "296ec54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN,self).__init__()\n",
    "        \n",
    "        self.conv_1 = nn.Conv2d(in_channels = 3,\n",
    "                               out_channels = 32,\n",
    "                               kernel_size = 3,\n",
    "                               stride = 1,\n",
    "                               padding = 1)\n",
    "        \n",
    "        self.activation = nn.ReLU()\n",
    "        \n",
    "        #class torch.nn.MaxPool2d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(kernel_size = 2,stride = 2)\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc_1 = nn.Linear(32*16*16,10) #32*16*16 = 8192, 10\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv_1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = self.flatten(x)\n",
    "        \n",
    "        x = self.fc_1(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "model = SimpleCNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "49e9abfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Neural Networks\n",
    "\n",
    "# OPTIM: Contains optimization algorithms like SGD, Adam, and RMSprop.\n",
    "# You use this to update your model's weights based on the calculated gradients.\n",
    "import torch.optim as optim\n",
    "\n",
    "# FUNCTIONAL: Contains \"stateless\" functions that don't hold weights.\n",
    "# Commonly used for activation functions (F.relu), loss functions (F.cross_entropy), \n",
    "# and pooling operations that don't need to store internal parameters.\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e87c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    " =========================================================================\n",
    "                     KINGS OF LOSS FUNCTIONS (CRITERIA)\n",
    " =========================================================================\n",
    "\n",
    " --- REGRESSION (Predicting continuous numbers) ---\n",
    "\n",
    " 1. nn.MSELoss() \n",
    " Use: Default for regression. Penalizes large errors heavily (squares them).\n",
    " Case: House prices, stock values, temperature.\n",
    "\n",
    " 2. nn.L1Loss()\n",
    " Use: Robust to outliers. Doesn't freak out as much as MSE over \"crazy\" data.\n",
    " Case: When your dataset is \"noisy\" or has extreme outlier values.\n",
    "\n",
    " 3. nn.HuberLoss()\n",
    " Use: The hybrid. Acts like MSE for small errors and L1 for large ones.\n",
    " Case: Best for general regression where you want stability.\n",
    "\n",
    "\n",
    " --- CLASSIFICATION (Predicting categories/labels) ---\n",
    "\n",
    " ⭐ 4. nn.CrossEntropyLoss()\n",
    " Use: The Multi-Class King. Combines LogSoftmax and NLLLoss in one step.\n",
    " Case: Image classification (0-9 digits, Cat vs Dog vs Bird). \n",
    " Note: Do NOT add a Softmax layer at the end of your model if you use this.\n",
    "\n",
    " ⭐ 5. nn.BCEWithLogitsLoss()\n",
    " Use: Binary Cross Entropy. For 2-choice problems.\n",
    " Case: Yes/No, Spam/Not Spam, or predicting multiple labels for one image.\n",
    "\n",
    " 6. nn.NLLLoss() (Negative Log Likelihood)\n",
    " Use: Only if you manually put 'F.log_softmax' at the end of your model.\n",
    " Case: Advanced classification where you need the log-probabilities.\n",
    "\n",
    "\n",
    " --- ADVANCED / SPECIALIZED ---\n",
    "\n",
    " 7. nn.KLDivLoss() (Kullback-Leibler Divergence)\n",
    " Use: Measures how one probability distribution differs from another.\n",
    " Case: Knowledge Distillation or Variational Autoencoders (VAEs).\n",
    "\n",
    " 8. nn.TripletMarginLoss()\n",
    " Use: Similarity learning. Makes \"friends\" closer and \"enemies\" farther apart.\n",
    " Case: Face recognition (FaceID), Signature verification.\n",
    "\n",
    " 9. nn.CTCLoss() (Connectionist Temporal Classification)\n",
    " Use: Predicting sequences when lengths don't match.\n",
    " Case: Speech-to-text, Handwriting recognition. \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bfaa06cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a loss function\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1ef51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    " =========================================================================\n",
    "                       KINGS OF OPTIMIZERS (THE DRIVERS)\n",
    " =========================================================================\n",
    "\n",
    " 1. optim.SGD (Stochastic Gradient Descent)\n",
    " Use: The \"Old School\" classic. Reliable but can be slow.\n",
    " Case: Often used with 'momentum=0.9' to help it roll over small bumps in the loss.\n",
    " Bro-Tip: Many top-tier researchers still use this for final \"polishing\" of models.\n",
    "\n",
    " 2. optim.Adam (Adaptive Moment Estimation)\n",
    " Use: The \"Default\" King. Usually the best starting point for any project.\n",
    " Case: Handles different learning rates for different parameters automatically.\n",
    " Bro-Tip: Use this if you want results fast without tweaking much.\n",
    "\n",
    " 3. optim.AdamW (Adam with Weight Decay)\n",
    " Use: The \"Modern\" Adam. It fixes a math bug in how Adam handles weight decay.\n",
    " Case: Standard for Transformers and modern CNNs.\n",
    " Bro-Tip: If Adam is overfitting, switch to AdamW.\n",
    "\n",
    " 4. optim.RMSprop (Root Mean Square Propagation)\n",
    " Use: Great for \"unstable\" gradients.\n",
    " Case: Frequently used in Recurrent Neural Networks (RNNs) and Reinforcement Learning.\n",
    "\n",
    " 5. optim.Adagrad (Adaptive Gradient Algorithm)\n",
    " Use: Good for sparse data (where some features don't appear often).\n",
    " Case: Natural Language Processing (NLP) or recommendation systems.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "047054d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define an optimizer\n",
    "optimizer = optim.SGD(model.parameters(),lr = 0.001, momentum = 0.9)\n",
    "#class torch.optim.SGD(params, lr=0.001, momentum=0, dampening=0, weight_decay=0, nesterov=False, *, maximize=False, foreach=None, differentiable=False, fused=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "56223ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dummy Datasset\n",
    "\n",
    "# inputs: Simulates a batch of 64 color images.\n",
    "# Shape: [64, 3, 32, 32] \n",
    "# -> 64: Batch size (how many images we show the AI at once)\n",
    "# -> 3:   Channels (Red, Green, Blue)\n",
    "# -> 32, 32: Height and Width in pixels\n",
    "inputs = torch.randn(64, 3, 32, 32) \n",
    "\n",
    "# labels: Simulates the \"correct answers\" for those 100 images.\n",
    "# torch.randint(0, 10, (64,))\n",
    "# -> 0: The lowest possible category index\n",
    "# -> 10: The highest category index (exclusive, so 0-9)\n",
    "# -> (64,): A 1D list of 64 random integers (one for each input image)\n",
    "labels = torch.randint(0, 10, (64,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "577910c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 10] loss: 2.047\n",
      "[1, 20] loss: 1.175\n",
      "[1, 30] loss: 0.474\n",
      "[1, 40] loss: 0.199\n",
      "[1, 50] loss: 0.109\n",
      "[1, 60] loss: 0.075\n",
      "[2, 10] loss: 0.054\n",
      "[2, 20] loss: 0.046\n",
      "[2, 30] loss: 0.041\n",
      "[2, 40] loss: 0.036\n",
      "[2, 50] loss: 0.033\n",
      "[2, 60] loss: 0.030\n",
      "[3, 10] loss: 0.027\n",
      "[3, 20] loss: 0.025\n",
      "[3, 30] loss: 0.023\n",
      "[3, 40] loss: 0.022\n",
      "[3, 50] loss: 0.021\n",
      "[3, 60] loss: 0.020\n",
      "[4, 10] loss: 0.018\n",
      "[4, 20] loss: 0.017\n",
      "[4, 30] loss: 0.016\n",
      "[4, 40] loss: 0.016\n",
      "[4, 50] loss: 0.015\n",
      "[4, 60] loss: 0.014\n",
      "[5, 10] loss: 0.014\n",
      "[5, 20] loss: 0.013\n",
      "[5, 30] loss: 0.013\n",
      "[5, 40] loss: 0.012\n",
      "[5, 50] loss: 0.012\n",
      "[5, 60] loss: 0.011\n",
      "[6, 10] loss: 0.011\n",
      "[6, 20] loss: 0.011\n",
      "[6, 30] loss: 0.010\n",
      "[6, 40] loss: 0.010\n",
      "[6, 50] loss: 0.010\n",
      "[6, 60] loss: 0.009\n",
      "[7, 10] loss: 0.009\n",
      "[7, 20] loss: 0.009\n",
      "[7, 30] loss: 0.009\n",
      "[7, 40] loss: 0.008\n",
      "[7, 50] loss: 0.008\n",
      "[7, 60] loss: 0.008\n",
      "[8, 10] loss: 0.008\n",
      "[8, 20] loss: 0.008\n",
      "[8, 30] loss: 0.007\n",
      "[8, 40] loss: 0.007\n",
      "[8, 50] loss: 0.007\n",
      "[8, 60] loss: 0.007\n",
      "[9, 10] loss: 0.007\n",
      "[9, 20] loss: 0.007\n",
      "[9, 30] loss: 0.007\n",
      "[9, 40] loss: 0.006\n",
      "[9, 50] loss: 0.006\n",
      "[9, 60] loss: 0.006\n",
      "[10, 10] loss: 0.006\n",
      "[10, 20] loss: 0.006\n",
      "[10, 30] loss: 0.006\n",
      "[10, 40] loss: 0.006\n",
      "[10, 50] loss: 0.006\n",
      "[10, 60] loss: 0.006\n"
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "\n",
    "for epoch in range(10):\n",
    "    running_loss = 0.0\n",
    "    for i in range(64):\n",
    "        # Placeholder to show where data enters; usually used to move data to GPU/CPU.\n",
    "        inputs, labels = inputs, labels #these data are randomly generated in our case \n",
    "        \n",
    "        optimizer.zero_grad() # Clears previous gradients so they don't accumulate.\n",
    "        \n",
    "        outputs = model(inputs) # Forward pass: feeds inputs to the model to get predictions.\n",
    "        loss = criterion(outputs, labels) # Calculates the error between predictions and actual labels.\n",
    "        \n",
    "        loss.backward() # Computes the gradient (how to change weights) using backpropagation.\n",
    "        optimizer.step() # Updates the model parameters based on the computed gradients.\n",
    "        \n",
    "        running_loss = running_loss + loss.item() # Adds the current batch's loss to the total.\n",
    "        \n",
    "        if i % 10 == 9:\n",
    "            print(f\"[{epoch + 1}, {i + 1}] loss: {running_loss / 10:.3f}\")\n",
    "            running_loss = 0.0 # Resets the counter to track the loss for the next 10 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "57aa3b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Random Test Data\n",
    "# We'll simulate 500 test images (5 batches of 100)\n",
    "test_inputs = torch.randn(50, 3, 32, 32)\n",
    "test_labels = torch.randint(0, 10, (50,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "467a54f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on random test images is: 8.00%\n"
     ]
    }
   ],
   "source": [
    "# Evaluation Setup\n",
    "correct = 0\n",
    "total = 0\n",
    "batch_size = 16\n",
    "\n",
    "# We use torch.no_grad() because we aren't training (saves memory/compute)\n",
    "with torch.no_grad():\n",
    "    # We iterate through our dummy test data in chunks/batches\n",
    "    for i in range(0, 50, batch_size):\n",
    "        # Slicing the data to simulate a loader\n",
    "        images = test_inputs[i : i + batch_size]\n",
    "        labels = test_labels[i : i + batch_size]\n",
    "        \n",
    "        # Forward pass\n",
    "        # Note: using 'model' to match your training variable name\n",
    "        outputs = model(images) \n",
    "        \n",
    "        # Get the index of the highest score (the prediction)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "# 3. Final Output\n",
    "accuracy = 100 * correct / total\n",
    "print(f\"Accuracy of the network on random test images is: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c69c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since this test was done on random data model accuracy is low"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
