{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7835efc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/ava-orange-education/Mastering-Computer-Vision-with-PyTorch-2.0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "341a6556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet in PyTorch\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad0a04f",
   "metadata": {},
   "source": [
    "<img src=\"images/Resnet-2.png\" width=1000 height=400/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4b9a54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        # First convolution: handles spatial downsampling via 'stride'\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
    "        # Normalize activations to speed up training and stabilize gradients\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        # Activation function (inplace=True saves memory)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        # Second convolution: always keeps the spatial dimensions the same (stride=1)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        # Second normalization layer\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        # If the input shape doesn't match the output, this layer fixes the 'shortcut' shape\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 1. Save the original input as the 'identity' or 'residual'\n",
    "        residual = x\n",
    "        \n",
    "        # 2. If the dimensions changed (stride > 1), adjust the residual shape\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "        \n",
    "        # 3. Main Path: Conv -> BN -> ReLU -> Conv -> BN\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        \n",
    "        # 4. THE MAGIC: Add the original input (residual) back to the output\n",
    "        # This is the \"Skip Connection\" or \"Short-cut\"\n",
    "        out += residual\n",
    "        \n",
    "        # 5. Final activation after the addition\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a25d298b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNet, self).__init__()\n",
    "        # Internal state to track the number of input channels for the next layer\n",
    "        self.in_channels = 64\n",
    "        \n",
    "        # --- THE STEM: Initial processing of the raw image ---\n",
    "        # Large 7x7 filter to capture initial spatial features; reduces resolution by half\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        # Maxpooling further reduces the spatial size (image becomes smaller and deeper)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        # --- THE TRUNK: Stacking residual stages ---\n",
    "        # layer1: 64 channels, stays the same size\n",
    "        self.layer1 = self.make_layer(64, 2)\n",
    "        # layer2: doubles channels to 128, reduces image size (stride=2)\n",
    "        self.layer2 = self.make_layer(128, 2, 2)\n",
    "        # layer3: doubles channels to 256, reduces image size (stride=2)\n",
    "        self.layer3 = self.make_layer(256, 2, 2)\n",
    "        # layer4: doubles channels to 512, reduces image size (stride=2)\n",
    "        self.layer4 = self.make_layer(512, 2, 2)\n",
    "\n",
    "        # --- THE HEAD: Classification ---\n",
    "        # Squeezes every feature map into a 1x1 pixel (Global Average Pooling)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        # Final fully connected layer mapping 512 features to 10 classes\n",
    "        self.fc = nn.Linear(512, 10)\n",
    "\n",
    "    def make_layer(self, out_channels, blocks, stride=1):\n",
    "        \"\"\"Creates a sequence of ResidualBlocks.\"\"\"\n",
    "        downsample = None\n",
    "        # Check if the shortcut path needs to resize the input to match the main path\n",
    "        if stride != 1 or self.in_channels != out_channels:\n",
    "            downsample = nn.Sequential(\n",
    "                # Use 1x1 conv to change channel depth and/or spatial resolution\n",
    "                nn.Conv2d(self.in_channels, out_channels, kernel_size=1, stride=stride),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        # The first block handles the change in dimensions (downsampling)\n",
    "        layers.append(ResidualBlock(self.in_channels, out_channels, stride, downsample))\n",
    "        # Update in_channels so subsequent blocks in this layer know the depth\n",
    "        self.in_channels = out_channels\n",
    "        \n",
    "        # Add the remaining blocks (which don't change dimensions)\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(ResidualBlock(out_channels, out_channels))\n",
    "        \n",
    "        # Pack the list of blocks into a PyTorch Sequential container\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 1. Pass through the initial 'stem'\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.maxpool(out)\n",
    "\n",
    "        # 2. Pass through the 4 stages of residual blocks\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "\n",
    "        # 3. Global average pooling (converts [Batch, 512, H, W] to [Batch, 512, 1, 1])\n",
    "        out = self.avgpool(out)\n",
    "        \n",
    "        # 4. Flatten the tensor for the Linear layer\n",
    "        out = out.view(out.size(0), -1)\n",
    "        \n",
    "        # 5. Get the class scores\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055a9c37",
   "metadata": {},
   "source": [
    "# NN VISUALIZATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a133efe",
   "metadata": {},
   "source": [
    "# 1. Using https://netron.app/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f0de1e",
   "metadata": {},
   "source": [
    "<img src=\"images/netron_app.png\" width=200 height=400/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f60a1f7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model exported successfully to 'my_resnet.onnx'\n"
     ]
    }
   ],
   "source": [
    "#Plot NN in https://netron.app/ by exporting model\n",
    "#           https://netron.app/\n",
    "\n",
    "import torch\n",
    "\n",
    "# 1. Initialize your model\n",
    "model = ResNet()\n",
    "model.eval() # Set to evaluation mode (crucial for Batch Norm layers)\n",
    "\n",
    "# 2. Create a dummy input\n",
    "# This is needed because ONNX actually \"runs\" the model once to trace the path\n",
    "dummy_input = torch.randn(1, 3, 32, 32) \n",
    "\n",
    "# 3. Export to ONNX\n",
    "torch.onnx.export(\n",
    "    model,                  # model being run\n",
    "    dummy_input,            # model input (or a tuple for multiple inputs)\n",
    "    \"my_resnet.onnx\",       # where to save the model\n",
    "    input_names = ['Input Image'],   # optional: name your input node\n",
    "    output_names = ['Class Scores']  # optional: name your output node\n",
    ")\n",
    "\n",
    "print(\"Model exported successfully to 'my_resnet.onnx'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb80c56",
   "metadata": {},
   "source": [
    "# 2. Using Tensor Board"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff23231",
   "metadata": {},
   "source": [
    "<img src=\"images/tensorboard.png\" width=1200 height=400/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "937062ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot NN using Tensor Board\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Create a TensorBoard writer to log metrics and model graph\n",
    "writer = SummaryWriter(log_dir='./logs')\n",
    "\n",
    "# If you have a random input tensor, pass it through the model to log the graph\n",
    "# Example input size: (batch_size, channels, height, width)\n",
    "example_input = torch.rand(1, 3, 224, 224)  # Example for a batch of size 1, RGB image 224x224\n",
    "writer.add_graph(model, example_input)\n",
    "\n",
    "# Now, let's define a simple training loop to log metrics as well\n",
    "\n",
    "# Example: Training loop\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Dummy dataset (replace with your real data)\n",
    "# x_train, y_train = your_dataset\n",
    "\n",
    "# Dummy training loop (replace with actual loop)\n",
    "for epoch in range(10):\n",
    "    # Forward pass (example)\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(example_input)  # Replace with actual inputs\n",
    "    loss = criterion(outputs, torch.randint(0, 10, (1,)))  # Dummy target (replace with actual labels)\n",
    "\n",
    "    # Backward pass and optimize\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Log the loss and other metrics to TensorBoard\n",
    "    writer.add_scalar('Loss/train', loss.item(), epoch)\n",
    "\n",
    "    # Optionally, log histograms of model parameters\n",
    "    for name, param in model.named_parameters():\n",
    "        writer.add_histogram(name, param, epoch)\n",
    "\n",
    "# Close the TensorBoard writer after training is complete\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "acd99849",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' in terminal hit \\n\\n            tensorboard --logdir=./logs\\n\\nthen in browset go to\\n\\n            http://localhost:6006/\\n\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' in terminal hit \n",
    "\n",
    "            tensorboard --logdir=./logs\n",
    "\n",
    "then in browset go to\n",
    "\n",
    "            http://localhost:6006/\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0eeb1f",
   "metadata": {},
   "source": [
    "# 3. Using https://graphviz.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c537a2",
   "metadata": {},
   "source": [
    "<img src=\"images/graphviz.png\" width=1200 height=400/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e56d81fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://graphviz.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6a4ab3",
   "metadata": {},
   "source": [
    "# 4. Using LaTeX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95b44ab",
   "metadata": {},
   "source": [
    "<img src=\"images/latex.png\" width=1200 height=400/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d9f118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/HarisIqbal88/PlotNeuralNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894e6fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# --- ADD THIS BLOCK AT THE VERY TOP ---\n",
    "# We point Python directly to the folder containing 'pycore'\n",
    "project_path = '/home/anonymous/Downloads/Books/Mastering Computer Vision-with Pytorch 2.0/PlotNeuralNet/'\n",
    "sys.path.append(project_path)\n",
    "# --------------------------------------\n",
    "\n",
    "from pycore.tikzeng import *\n",
    "\n",
    "# Define the Architecture\n",
    "arch = [\n",
    "    to_head('..'),\n",
    "    to_cor(),\n",
    "    to_begin(),\n",
    "\n",
    "    # --- 0. INPUT IMAGE ---\n",
    "    to_input('images/input.png', width=6, height=6, name=\"input\"),\n",
    "\n",
    "    # --- 1. INITIAL BLOCK (Stem) ---\n",
    "    # Conv 7x7, stride 2 (Gray/Yellowish)\n",
    "    to_Conv(\"conv1\", 64, 64, offset=\"(0,0,0)\", to=\"(0,0,0)\", height=32, depth=32, width=2, caption=\"Stem\"),\n",
    "    to_Pool(\"pool1\", offset=\"(0,0,0)\", to=\"(conv1-east)\", caption=\"Pool\"),\n",
    "\n",
    "    # --- 2. LAYER 1 (Residual Block 1) ---\n",
    "    # Input: 32x32, 64ch -> Output: 32x32, 64ch (No size change)\n",
    "    to_Conv(\"l1_conv1\", 64, 64, offset=\"(2,0,0)\", to=\"(pool1-east)\", height=32, depth=32, width=2, caption=\"L1_Conv1\"),\n",
    "    to_Conv(\"l1_conv2\", 64, 64, offset=\"(0,0,0)\", to=\"(l1_conv1-east)\", height=32, depth=32, width=2, caption=\"L1_Conv2\"),\n",
    "    \n",
    "    # Connection (Main Path)\n",
    "    to_connection(\"pool1\", \"l1_conv1\"), \n",
    "    \n",
    "    # Skip Connection (Curve from start of L1 to end of L1)\n",
    "    to_skip(of='l1_conv1', to='l1_conv2', pos=1.25), \n",
    "\n",
    "\n",
    "    # --- 3. LAYER 2 (Residual Block 2) ---\n",
    "    # Downsample! Size 32->16, Channels 64->128\n",
    "    # We visually make the block smaller (height=16) and thicker (width=4)\n",
    "    to_Conv(\"l2_conv1\", 128, 16, offset=\"(2,0,0)\", to=\"(l1_conv2-east)\", height=16, depth=16, width=4, caption=\"L2_Conv1\"),\n",
    "    to_Conv(\"l2_conv2\", 128, 16, offset=\"(0,0,0)\", to=\"(l2_conv1-east)\", height=16, depth=16, width=4, caption=\"L2_Conv2\"),\n",
    "    \n",
    "    to_connection(\"l1_conv2\", \"l2_conv1\"),\n",
    "    # Skip Connection\n",
    "    to_skip(of='l2_conv1', to='l2_conv2', pos=1.25),\n",
    "\n",
    "\n",
    "    # --- 4. LAYER 3 (Residual Block 3) ---\n",
    "    # Downsample! Size 16->8, Channels 128->256\n",
    "    # Smaller (height=8) and even thicker (width=7)\n",
    "    to_Conv(\"l3_conv1\", 256, 8, offset=\"(2,0,0)\", to=\"(l2_conv2-east)\", height=8, depth=8, width=7, caption=\"L3_Conv1\"),\n",
    "    to_Conv(\"l3_conv2\", 256, 8, offset=\"(0,0,0)\", to=\"(l3_conv1-east)\", height=8, depth=8, width=7, caption=\"L3_Conv2\"),\n",
    "    \n",
    "    to_connection(\"l2_conv2\", \"l3_conv1\"),\n",
    "    to_skip(of='l3_conv1', to='l3_conv2', pos=1.25),\n",
    "\n",
    "\n",
    "    # --- 5. FINAL CLASSIFICATION ---\n",
    "    # Global Avg Pool (represented as a small flat layer)\n",
    "    to_Pool(\"avg_pool\", offset=\"(2,0,0)\", to=\"(l3_conv2-east)\", height=1, depth=1, width=1, caption=\"AvgPool\"),\n",
    "    \n",
    "    # Fully Connected / Softmax (10 classes)\n",
    "    to_SoftMax(\"soft1\", 10, \"(3,0,0)\", \"(avg_pool-east)\", caption=\"FC 10\"),\n",
    "    \n",
    "    to_connection(\"l3_conv2\", \"avg_pool\"),\n",
    "    to_connection(\"avg_pool\", \"soft1\"),\n",
    "\n",
    "    to_end()\n",
    "]\n",
    "\n",
    "def main():\n",
    "    namefile = str(sys.argv[0]).split('.')[0]\n",
    "    to_generate(arch, namefile + '.tex')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
